{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrMWrnd8UPek"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqhXSRaij-sH"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7x-tZfIUXKy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from transformers import pipeline\n",
        "from transformers import BertTokenizer, BertModel,  AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchmetrics import F1\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmmZHDDM2gfF"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ues8mQwUefI"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42\n",
        "MAX_LEN = 300\n",
        "BATCH_SIZE=16\n",
        "NCLASSES= 3\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWyaYB_zas1F"
      },
      "outputs": [],
      "source": [
        "url_train = 'ttps://drive.google.com/file/d/1OuZDURMv7uA692UxR2nfgzGUi0qcyw2Y/view?usp=sharing'\n",
        "\n",
        "url_validation = 'https://drive.google.com/file/d/1kXG0kJl3_0NdnQmvBzYpvBcJEnXAFkcj/view?usp=sharing'\n",
        "\n",
        "url_test = 'https://drive.google.com/file/d/1DQM2OX-WAqPDsuQutJjgl7ebOKsmn7xp/view?usp=sharing'\n",
        "\n",
        "def path_download_csv(url):\n",
        "    return 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXQiaOPjUngd"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(path_download_csv(url_train))\n",
        "validation = pd.read_csv(path_download_csv(url_validation))\n",
        "test = pd.read_csv(path_download_csv(url_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeGZ7inF2Vo2"
      },
      "outputs": [],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHVvuhr6U3ep"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIcVP0ZoXTd2"
      },
      "outputs": [],
      "source": [
        "text = train['review'][2]\n",
        "tokens = tokenizer.tokenize(text)\n",
        "tokens_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(text, tokens, tokens_id, sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVMRdBH-X4ZC"
      },
      "outputs": [],
      "source": [
        "class AmazonDataset(Dataset):\n",
        "    def __init__(self, reviews, labels, tokenizer, max_len):\n",
        "        self.reviews = reviews\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        review = str(self.reviews[item]) \n",
        "        label = self.labels[item]\n",
        "        encoding = tokenizer.encode_plus(\n",
        "                                        review,\n",
        "                                        max_length=self.max_len,\n",
        "                                        truncation=True,\n",
        "                                        add_special_tokens=True,\n",
        "                                        return_token_type_ids=False,\n",
        "                                        padding='max_length',\n",
        "                                        return_attention_mask=True,\n",
        "                                        return_tensors='pt')\n",
        "        return{\n",
        "            'review':review,\n",
        "            'input_ids':encoding['input_ids'].flatten(),\n",
        "            'attention_mask':encoding['attention_mask'].flatten(),\n",
        "            'label':torch.tensor(label, dtype=torch.long)} "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZ0FJ2T_YdwJ"
      },
      "outputs": [],
      "source": [
        "def data_loader(df, tokenizer, max_len, batch_size):\n",
        "  dataset = AmazonDataset(\n",
        "    reviews=df.review.to_numpy(),\n",
        "    labels = df.label.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "  return DataLoader(dataset, batch_size= BATCH_SIZE, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp26gpGRYj8V"
      },
      "outputs": [],
      "source": [
        "train_data_loader = data_loader(train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "validation_data_loader = data_loader(validation, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = data_loader(test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdUn0P26Ynqb"
      },
      "outputs": [],
      "source": [
        "class Beto(nn.Module):\n",
        "    def __init__(self, n_class):\n",
        "        super(Beto, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
        "        self.do = nn.Dropout(0.5)\n",
        "        self.linear = nn.Linear(self.bert.config.hidden_size, n_class)\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, cls_output = self.bert(\n",
        "            input_ids = input_ids,\n",
        "            attention_mask = attention_mask,\n",
        "            return_dict=False\n",
        "        )\n",
        "        dropout = self.do(cls_output)\n",
        "        output = self.linear(dropout)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t9UW2oFYvvM"
      },
      "outputs": [],
      "source": [
        "model = Beto(NCLASSES)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTsZL2eUY1oV"
      },
      "outputs": [],
      "source": [
        "EPOCHS=5\n",
        "optimizer = AdamW (model.parameters(), lr=1e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader)*EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps = total_steps\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB7GGSd3by60"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "f1 = F1(num_classes=3).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9gkLJr1ZDWr"
      },
      "outputs": [],
      "source": [
        "def train_model (model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model=model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  f1_score_global = []\n",
        "  i = 0\n",
        "  for batch in data_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['label'].to(device)\n",
        "    outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    correct_predictions += torch.sum(preds == labels)\n",
        "    f1_score_global.append(f1(preds, labels).cpu().detach().numpy())\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    print('Ejemplo {}/{} , Entrenamiento: Loss: {}, mean f1: {}'.format(i, n_examples/BATCH_SIZE,loss, np.mean(f1_score_global)))\n",
        "    i+=1\n",
        "  return correct_predictions.double()/n_examples, np.mean(losses), np.mean(f1_score_global)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gzD3_REZIbx"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples, modo):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  f1_score_global = []\n",
        "  i=0\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['label'].to(device)\n",
        "      outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      f1_score_global.append(f1(preds, labels).cpu().detach().numpy())\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      correct_predictions += torch.sum(preds == labels)\n",
        "      losses.append(loss.item())\n",
        "      print('Ejemplo {}/{} , {}: Loss: {}, mean f1: {}'.format(i, n_examples//BATCH_SIZE, modo, loss, np.mean(f1_score_global)))\n",
        "      i+=1\n",
        "  return correct_predictions.double()/n_examples, np.mean(losses), np.mean(f1_score_global)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpBL5ciG_XWm"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'El modelo tiene {count_parameters(model)} de parámetros')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBXslKV1ZOns"
      },
      "outputs": [],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  print('Epoch {} de {}'.format(epoch+1, EPOCHS))\n",
        "  print('------------------')\n",
        "  train_acc, train_loss, train_f1 = train_model(\n",
        "    model, train_data_loader, loss_fn, optimizer, device, scheduler, len(train)\n",
        "  )\n",
        "  validation_acc, validation_loss, validation_f1 = eval_model(\n",
        "    model, validation_data_loader, loss_fn, device, len(validation), 'Validación'\n",
        "  )\n",
        "  checkpoint = {'epoch': epoch + 1, 'state_dict': model.state_dict(),\n",
        "             'optimizer': optimizer.state_dict()}\n",
        "  torch.save(checkpoint, f'checkpoint_{epoch+1}.pth')\n",
        "  print('Entrenamiento: Loss: {}, accuracy: {}, f1: {}'.format(train_loss, train_acc, train_f1))\n",
        "  print('Validación: Loss: {}, accuracy: {}, f1: {}'.format(validation_loss, validation_acc, validation_f1))\n",
        "  print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dH6cyYQaRfi"
      },
      "source": [
        "**<h1>Uso de Checkpoint</h1>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCvJUTXroPND"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dlfF1L8oa5H"
      },
      "outputs": [],
      "source": [
        "cd drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhVBDF2homv4"
      },
      "outputs": [],
      "source": [
        "mkdir checkpoint/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7gaRAiVouuD"
      },
      "outputs": [],
      "source": [
        "cd checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyDqzNE-pCep"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C9tfsD-F2xF"
      },
      "outputs": [],
      "source": [
        "loaded_checkpoint = torch.load('Copy of checkpoint_1.pth', map_location=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmUr8ymKbPhm"
      },
      "outputs": [],
      "source": [
        "last_epoch = loaded_checkpoint['epoch']\n",
        "optimizer.load_state_dict(loaded_checkpoint['optimizer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd2vwdKI_iuU"
      },
      "outputs": [],
      "source": [
        "optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD9U0etwF-V9"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(loaded_checkpoint['state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBpcxS619wwK"
      },
      "outputs": [],
      "source": [
        "torch.save(loaded_checkpoint['state_dict'],f'Beto_weight{last_epoch}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x2iAcm2blZc"
      },
      "outputs": [],
      "source": [
        "for epoch in range(last_epoch, EPOCHS):\n",
        "  print('Epoch {} de {}'.format(epoch+1, EPOCHS))\n",
        "  print('------------------')\n",
        "  train_acc, train_loss, train_f1 = train_model(\n",
        "    model, train_data_loader, loss_fn, optimizer, device, scheduler, len(train)\n",
        "  )\n",
        "  validation_acc, validation_loss, validation_f1 = eval_model(\n",
        "    model, validation_data_loader, loss_fn, device, len(validation),  'Validación'\n",
        "  )\n",
        "  checkpoint = {'epoch': epoch + 1, 'state_dict': model.state_dict(),\n",
        "             'optimizer': optimizer.state_dict()}\n",
        "  torch.save(checkpoint, f'checkpoint_{epoch+1}.pth')\n",
        "  print('Entrenamiento: Loss: {}, accuracy: {}, f1: {}'.format(train_loss, train_acc, train_f1))\n",
        "  print('Validación: Loss: {}, accuracy: {}, f1: {}'.format(validation_loss, validation_acc, validation_f1))\n",
        "  print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_34op-C67aS"
      },
      "source": [
        "**<h1>Test</h1>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1d2-cmz6fUn"
      },
      "outputs": [],
      "source": [
        "print('------------------')\n",
        "test_acc, test_loss, test_f1 = eval_model(\n",
        "        model, test_data_loader, loss_fn, device, len(test), 'Test'\n",
        "    )\n",
        "print('Test: Loss: {}, accuracy: {}, f1: {}'.format(test_loss, test_acc, test_f1))\n",
        "print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3zCbkgh7GaI"
      },
      "outputs": [],
      "source": [
        "def clasificacion_sentimiento(review):\n",
        "  encoding_review = tokenizer.encode_plus(\n",
        "      review,\n",
        "      max_length=MAX_LEN,\n",
        "      truncation=True,\n",
        "      add_special_tokens=True,\n",
        "      return_token_type_ids=False,\n",
        "      #pad_to_max_length=True,\n",
        "      padding='max_length',\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt'\n",
        "    )\n",
        "  \n",
        "  input_ids=encoding_review['input_ids'].to(device)\n",
        "  attention_mask=encoding_review['attention_mask'].to(device)\n",
        "  output = model(input_ids, attention_mask)\n",
        "  _, prediction = torch.max(output,dim=1)\n",
        "  if prediction==2:\n",
        "    print('Sentimiento positivo')\n",
        "  elif prediction==1:\n",
        "    print('Sentimiento neutro')  \n",
        "  elif prediction==0:\n",
        "    print('Sentimiento negativo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIdBSAOs-yVT"
      },
      "outputs": [],
      "source": [
        "clasificacion_sentimiento(\"Excelente experiencia Comida maravillosa con excelentes insumos y perfectas mezclas.\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SSI9lkm-0yM"
      },
      "outputs": [],
      "source": [
        "clasificacion_sentimiento(\"Muy buena presentación y servicio sin embargo exageradamente costoso y no se informa ennla carta.\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpAmvCdz-24R"
      },
      "outputs": [],
      "source": [
        "clasificacion_sentimiento('Los 330 soles (US$100) peor invertidos. Fui con mi hija a Cabrera de Miraflores, al llegar la reserva los trabajadores no supieron que hacer con lo que solicitamos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fh9ccM--94f"
      },
      "outputs": [],
      "source": [
        "clasificacion_sentimiento(\"la licuadora en general es buena pero tiene algunos fallos aun asi esta bien creo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFjU2pdn_A5L"
      },
      "outputs": [],
      "source": [
        "clasificacion_sentimiento('el producto esta ok eso creo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuzFsVU4pbWZ"
      },
      "outputs": [],
      "source": [
        "clasificacion_sentimiento('el producto esta ok eso creo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d4-NLpTqUW9"
      },
      "outputs": [],
      "source": [
        "clasificacion_sentimiento('genial, esta novela me servira para dormir mejor')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Beto.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
